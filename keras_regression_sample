library("keras")

#-----------read csv file as compounds--------------
compounds <- read.csv('../input/trial2/sampledata.csv')

#-----------remove some columns if needed--------------
trimed.compounds <- compounds[,]

#-----------select rows without empty cells---------
is.completes <- complete.cases(trimed.compounds)
is.completes

complete.compounds <- trimed.compounds[is.completes,]

#-----------select x from the dataset-----------------
x.0 <- complete.compounds[,c(3:35)]
x.sds <- apply(x.0, 2, sd)

sd.is.not.0 <- x.sds != 0 
x.0.s <- x.0[, (sd.is.not.0)]

#--------------------divide into test and training data----------------------
train_size = 0.6

n = nrow(x.0.s)
#------------collect the data with n*train_size from the dataset------------
perm = sample(n, size = round(n * train_size))

train_data <- cbind.data.frame(x.0.s[perm, ])
test_data <- cbind.data.frame(x.0.s[-perm, ])
nrow(test_data)
#-----------select y from the dataset------------------
train_labels <- complete.compounds[perm, c(2)]
test_labels <- complete.compounds[-perm, c(2)]

# Test data is *not* used when calculating the mean and std.

# Normalize training data
train_data <- scale(train_data) 

# Use means and standard deviations from training set to normalize test set
col_means_train <- attr(train_data, "scaled:center") 
col_stddevs_train <- attr(train_data, "scaled:scale")
test_data <- scale(test_data, center = col_means_train, scale = col_stddevs_train)

train_data[1, ] # First training sample, normalized

build_model <- function() {
  
  model <- keras_model_sequential() %>%
    layer_dense(units = 100, activation = "relu",
                input_shape = dim(train_data)[2]) %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.8) %>%
    layer_dense(units = 300, activation = "relu") %>%
    layer_batch_normalization() %>%
    layer_dropout(rate = 0.8) %>%
#    layer_dense(units = 200, activation = "relu") %>%
#    layer_batch_normalization() %>%
#    layer_dropout(rate = 0.8) %>%    
#    layer_dense(units = 100, activation = "relu") %>%
#    layer_batch_normalization() %>%
#    layer_dropout(rate = 0.75) %>%
    layer_dense(units = 1)
  
  model %>% compile(
    loss = "mse",
    optimizer = optimizer_adadelta(lr = initial_lr),
    metrics = list("mean_absolute_error")
  )
  
  model
}

epochs <- 1000
batch_size <- 128
initial_lr<-2
decay<-2
period<-25


model <- build_model()
model %>% summary()

# Display training progress by printing a single dot for each completed epoch.
print_dot_callback <- callback_lambda(
  on_epoch_end = function(epoch, logs) {
    if (epoch %% 80 == 0) cat("\n")
    cat(".")
  }
)    

lr_schedule<-function(epoch,lr) {
  lr=initial_lr/decay^((epoch-1)%%period)+1e-5
  lr
}
cb_lr<-callback_learning_rate_scheduler(lr_schedule)
early_stop <- callback_early_stopping(monitor = "val_loss", patience = 50)

# Fit the model and store training stats
history <- model %>% fit(
  train_data,
  train_labels,
  epochs = epochs,
  validation_split = 0.2,
  verbose = 0,
  batch_size = batch_size,
  callbacks = list(print_dot_callback, cb_lr, early_stop)
)

plot(history)

train_predictions <- model %>% predict(train_data)
plot(train_predictions, train_labels)
r2train <- cor(train_predictions, train_labels)**2
r2train

test_predictions <- model %>% predict(test_data)
plot(test_predictions, test_labels)
r2test <- cor(test_predictions, test_labels)**2
r2test
